{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e61b2533-256f-4824-bc36-cf1f9536d7f2",
   "metadata": {},
   "source": [
    "If we increase the context size (from 1 character to 2, for example), the model we built is going to get overly complicated. \\\n",
    "\\\n",
    "This notebook focuses on implementing a multi-layer perceptron character-level language model. \\\n",
    "\\\n",
    "**Number of features = number of dimensions** \\\n",
    "\\\n",
    "Randomly initialized vectors of words (or characters in this notebook) => tune the embeddings (vectors) using backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2adbc400-1f0b-4453-9b36-46b621a9f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adb45e2f-ae1e-4cd1-9735-806d9821db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "843ebe81-8b78-4cef-a920-76a2fee01f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4699784b-84d0-4807-a30b-46da7055acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f69a05c-1aaa-4f62-b469-8e935e6930a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: the number of characters we take to predict the next one\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    context = [0] * 3\n",
    "    for ch in w + '.':\n",
    "        idx = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(idx)\n",
    "        print(''.join(itos[i] for i in context), '--->', itos[idx])\n",
    "        context = context[1:] + [idx]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "662feab3-6fee-4286-afa4-147c4230c9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype # the dataset we built has 32 examples with 3 integers each and 32 integer labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1402a35b-e701-4cbb-b426-90ee8cb699d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7aeeed7a-b70f-46c2-8956-a0d4a7ed6108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5f15795-2acd-4662-8aae-5ae00cb75461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 27 characters to embed in a lower-dimensional space\n",
    "# in the paper (Bengio et al.), they embed 17000 words into 30d space\n",
    "# we can embed 27 characters in 2d\n",
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f59d10f4-e249-469b-8fb2-9c94737f0924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8140,  0.3182],\n",
       "        [ 0.5108, -0.7072],\n",
       "        [-0.2109, -0.9830]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[[5, 6, 7]] # the vectors of E, F, G in the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82cd8e29-488e-4413-a6b8-2e461674e7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can embed all of the integers like:\n",
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5990d8e8-c849-471d-af17-a314a3172d1d",
   "metadata": {},
   "source": [
    "### The hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adee0410-7efd-44a4-bdc0-55208089e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6, 100)) # weights of the hidden layer: we have three 2D embeddings, so the number of inputs = 6\n",
    "b1 = torch.randn(100) # number of neurons in the layer is variable, e.g. 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3e262d-baf2-4c7f-8058-8afa051b781e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "emb @ W1 + b1 matrix multiplication does not work because the dimensions don't match, so we need to concatenate the input layer, or more efficiently, \"view\" it in another dimension (constant space complexity).\n",
    "\n",
    "**(32, 3, 2) => (32, 6)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c1913f1-b0cf-47f9-8aa7-baa34722f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # tanh activation layer, we'll have numbers between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb003e9f-c9fa-418e-860f-17dd42477729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f8225f-8685-40fe-8f89-d5f01208d64a",
   "metadata": {},
   "source": [
    "### The output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10a82c61-26f5-4973-ac0a-858089a23b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6143579a-c8ee-405c-9ac3-cdf84a7ae296",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f03d178-1cbc-47d1-b12b-1e51825f55f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
