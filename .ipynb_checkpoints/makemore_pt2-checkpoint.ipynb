{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e383d883-1e90-4d14-9dc2-7ae6f5e9b966",
   "metadata": {},
   "source": [
    "The neural network approach of makemore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0ef0a5aa-61db-4155-97bd-0ac2dfccea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the names.txt dataset\n",
    "words = open('names.txt', 'r').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f3fe9f62-e4e5-460b-a5d2-beea3a76c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bdc34969-d089-49e4-bfa4-7ad4478463f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d9b9fcb5-91c9-47c4-aaf8-b564479a8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training set of bigrams (x, y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]: \n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        xs.append(idx1)\n",
    "        ys.append(idx2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1a4d748c-5a29-4d7b-b901-6ff62964cc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  5, 13, 13,  1]), tensor([ 5, 13, 13,  1,  0]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc5531-6d29-4fe8-a18c-30a2abeb9907",
   "metadata": {},
   "source": [
    "Two lists of corresponding indices. For example, when the input is integer 0, we want the weights to be arranged so that integer 5 gets a high probability. \\\n",
    "The input neuron as a vector (rather than an integer like 5) makes more sentence, so we want to one-hot encode them. So integer 5 will be a vector of (27, 1) dimension with all zeroes and one 1 at index 5. \\\n",
    "This is going to be done with torch.nn.functional.one_hot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "97d3da3b-f6f6-4062-8505-832b82fc7c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # x encoded\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2e5de4e4-e385-44ca-ae14-be69508621fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1042de030>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a31f0162-7aed-4031-9f53-d25b5c57ae3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.dtype # must be float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "95db1769-1128-413e-9b6c-9e51c2d67649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8038,  0.0093, -0.0675,  0.8845,  0.9397, -0.7330, -0.5890,  0.3042,\n",
       "          0.9441, -1.4434, -0.2408, -0.6050, -0.1692, -0.3387, -0.2628,  0.5830,\n",
       "          1.4813,  0.3213, -0.9092,  0.2592,  1.6155, -0.3437, -0.4232,  0.0751,\n",
       "          0.2564,  0.0695, -1.2382],\n",
       "        [ 0.2633, -0.8815,  1.2056, -0.5360, -0.6808,  1.8133, -0.9199, -0.2598,\n",
       "          0.3206, -1.2067, -0.2267,  0.2135, -1.0253, -0.3873, -0.9467, -1.6198,\n",
       "         -1.9078,  0.2680, -0.4910,  0.3689, -0.5442, -0.0502, -0.4051,  0.3407,\n",
       "          0.6713,  1.1889,  0.1951],\n",
       "        [-1.1106,  0.9162,  1.2149,  0.3994, -0.5210,  0.2556, -1.2713, -0.3430,\n",
       "          0.0551, -0.5630,  0.6466, -0.4448,  0.6445,  1.1203,  1.0656,  0.0833,\n",
       "          1.2658,  1.0735,  0.1648,  0.4858, -0.3074,  0.1759,  0.4995,  0.1997,\n",
       "         -1.2628,  2.0009,  1.2163],\n",
       "        [-1.1106,  0.9162,  1.2149,  0.3994, -0.5210,  0.2556, -1.2713, -0.3430,\n",
       "          0.0551, -0.5630,  0.6466, -0.4448,  0.6445,  1.1203,  1.0656,  0.0833,\n",
       "          1.2658,  1.0735,  0.1648,  0.4858, -0.3074,  0.1759,  0.4995,  0.1997,\n",
       "         -1.2628,  2.0009,  1.2163],\n",
       "        [ 0.4933,  0.7145, -0.6414, -1.9943,  0.3910, -1.1145,  0.1200,  0.7314,\n",
       "         -0.2904, -1.1797,  0.7532,  0.8731,  0.9848,  0.8017,  0.4493,  0.4199,\n",
       "          1.4530, -1.6072, -1.2509, -0.8545,  1.0413, -0.6763, -0.1895,  0.3466,\n",
       "          1.2886, -1.4176, -0.9791]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2149582346)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True) # weights, randn fills the tensor with random numbers from a standard normal distribution\n",
    "xenc @ W # @ is a matrix multiplication operator in pytorch, so we're multiplying inputs by weights (y = xw + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2687f5-0e8d-4efb-a845-44a3aae350ca",
   "metadata": {},
   "source": [
    "The output is (5, 27). We want these numbers to represent probabilities, but instead of using counts, we'll use log-counts. Take the numbers and exponentiate them, so negative numbers become numbers below zero and positive numbers become numbers above one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "09e46d6c-f8ae-4090-8879-3fad2b0496fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0597, 0.0270, 0.0250, 0.0647, 0.0684, 0.0128, 0.0148, 0.0362, 0.0687,\n",
       "         0.0063, 0.0210, 0.0146, 0.0226, 0.0191, 0.0206, 0.0479, 0.1176, 0.0369,\n",
       "         0.0108, 0.0346, 0.1345, 0.0190, 0.0175, 0.0288, 0.0345, 0.0287, 0.0077],\n",
       "        [0.0402, 0.0128, 0.1032, 0.0181, 0.0156, 0.1895, 0.0123, 0.0238, 0.0426,\n",
       "         0.0092, 0.0246, 0.0383, 0.0111, 0.0210, 0.0120, 0.0061, 0.0046, 0.0404,\n",
       "         0.0189, 0.0447, 0.0179, 0.0294, 0.0206, 0.0435, 0.0605, 0.1015, 0.0376],\n",
       "        [0.0067, 0.0512, 0.0690, 0.0305, 0.0122, 0.0264, 0.0057, 0.0145, 0.0216,\n",
       "         0.0117, 0.0391, 0.0131, 0.0390, 0.0628, 0.0594, 0.0223, 0.0726, 0.0599,\n",
       "         0.0241, 0.0333, 0.0151, 0.0244, 0.0337, 0.0250, 0.0058, 0.1515, 0.0691],\n",
       "        [0.0067, 0.0512, 0.0690, 0.0305, 0.0122, 0.0264, 0.0057, 0.0145, 0.0216,\n",
       "         0.0117, 0.0391, 0.0131, 0.0390, 0.0628, 0.0594, 0.0223, 0.0726, 0.0599,\n",
       "         0.0241, 0.0333, 0.0151, 0.0244, 0.0337, 0.0250, 0.0058, 0.1515, 0.0691],\n",
       "        [0.0432, 0.0538, 0.0139, 0.0036, 0.0390, 0.0086, 0.0297, 0.0548, 0.0197,\n",
       "         0.0081, 0.0560, 0.0631, 0.0706, 0.0588, 0.0413, 0.0401, 0.1127, 0.0053,\n",
       "         0.0075, 0.0112, 0.0747, 0.0134, 0.0218, 0.0373, 0.0956, 0.0064, 0.0099]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W # log-counts\n",
    "counts = logits.exp() # equivalent to the N table from makemore\n",
    "probs = counts / counts.sum(1, keepdims = True)\n",
    "# these two lines above are called softmax\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4d4afe6f-85f3-4c50-bda2-969444d39c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbce0f-3443-4005-864b-abe8f90e8e6a",
   "metadata": {},
   "source": [
    "So for each example from the dataset, we tooks the indices, one-hot encoded them, then it went to the neural net and we got the distribution of probabilities for the next character out of the neural net. \\\n",
    "Softmax: layer that takes the logits, exponentiates them, and normalizes. Basically taking the outputs of a neural net layer and outputs a probability distribution.\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{e^{z_i}}{\\sum^K_{j=1}{e^{z_i}}}\n",
    "\\end{aligned}\n",
    "\n",
    "We can backpropagate through all these layers. \\\n",
    "\\\n",
    "Similar to micrograd, we can evaluate the loss and try to minimize it, but instead of using the mean-squared error (MSE), we'll use the negative log-likelihood. \\\n",
    "We can get the probabilities into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "407204d1-3d2b-4e72-a02a-e0f1dcf3060e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0128, 0.0210, 0.0628, 0.0512, 0.0432], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[torch.arange(5), ys] # gets the probabilities corresponding to ys from 5 probs vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "08051d82-1009-42ec-935d-7edc4a4023f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4204, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -probs[torch.arange(5), ys].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "80436779-0d2d-403f-b288-55273370537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "W.grad = None # same as setting the gradient to zero \n",
    "loss.backward() # PyTorch builds the whole graph and knows all dependencies and operations, so now it fills in the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d39fe6ec-b2da-42ea-b5e7-6cb22fd67ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0119,  0.0054,  0.0050,  0.0129,  0.0137, -0.1974,  0.0030,  0.0072,\n",
       "          0.0137,  0.0013,  0.0042,  0.0029,  0.0045,  0.0038,  0.0041,  0.0096,\n",
       "          0.0235,  0.0074,  0.0022,  0.0069,  0.0269,  0.0038,  0.0035,  0.0058,\n",
       "          0.0069,  0.0057,  0.0015],\n",
       "        [-0.1914,  0.0108,  0.0028,  0.0007,  0.0078,  0.0017,  0.0059,  0.0110,\n",
       "          0.0039,  0.0016,  0.0112,  0.0126,  0.0141,  0.0118,  0.0083,  0.0080,\n",
       "          0.0225,  0.0011,  0.0015,  0.0022,  0.0149,  0.0027,  0.0044,  0.0075,\n",
       "          0.0191,  0.0013,  0.0020],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0080,  0.0026,  0.0206,  0.0036,  0.0031,  0.0379,  0.0025,  0.0048,\n",
       "          0.0085,  0.0018,  0.0049,  0.0077,  0.0022, -0.1958,  0.0024,  0.0012,\n",
       "          0.0009,  0.0081,  0.0038,  0.0089,  0.0036,  0.0059,  0.0041,  0.0087,\n",
       "          0.0121,  0.0203,  0.0075],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0027, -0.1795,  0.0276,  0.0122,  0.0049,  0.0106,  0.0023,  0.0058,\n",
       "          0.0087,  0.0047,  0.0156,  0.0053,  0.0156, -0.1749,  0.0238,  0.0089,\n",
       "          0.0290,  0.0240,  0.0097,  0.0133,  0.0060,  0.0098,  0.0135,  0.0100,\n",
       "          0.0023,  0.0606,  0.0276],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad # every element grad tells the influence of that element on the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "39b7739f-f693-4071-8c29-93fe04e378d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for w in words: \n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        xs.append(idx1)\n",
    "        ys.append(idx2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "num = xs.nelement() # number of examples of bigrams\n",
    "\n",
    "g = torch.Generator().manual_seed(257483234)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b8dffb1f-c5d4-441b-9205-1ae648193d0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.712122678756714\n",
      "3.3778560161590576\n",
      "3.159420967102051\n",
      "3.0192062854766846\n",
      "2.9230029582977295\n",
      "2.852320909500122\n",
      "2.799591064453125\n",
      "2.7593419551849365\n",
      "2.7277028560638428\n",
      "2.702122211456299\n",
      "2.6809582710266113\n",
      "2.663135528564453\n",
      "2.6479201316833496\n",
      "2.6347856521606445\n",
      "2.623340368270874\n",
      "2.613283157348633\n",
      "2.604379177093506\n",
      "2.5964436531066895\n",
      "2.5893287658691406\n",
      "2.582916259765625\n",
      "2.5771079063415527\n",
      "2.571824550628662\n",
      "2.5669991970062256\n",
      "2.5625760555267334\n",
      "2.558507204055786\n",
      "2.5547521114349365\n",
      "2.5512759685516357\n",
      "2.5480496883392334\n",
      "2.545046806335449\n",
      "2.542245388031006\n",
      "2.5396265983581543\n",
      "2.537172555923462\n",
      "2.53486967086792\n",
      "2.5327045917510986\n",
      "2.5306661128997803\n",
      "2.5287435054779053\n",
      "2.526928186416626\n",
      "2.525212287902832\n",
      "2.523588180541992\n",
      "2.52204966545105\n",
      "2.520590305328369\n",
      "2.519205093383789\n",
      "2.5178885459899902\n",
      "2.5166373252868652\n",
      "2.5154459476470947\n",
      "2.5143113136291504\n",
      "2.5132298469543457\n",
      "2.5121982097625732\n",
      "2.511213541030884\n",
      "2.5102732181549072\n",
      "2.5093741416931152\n",
      "2.508514642715454\n",
      "2.5076920986175537\n",
      "2.5069046020507812\n",
      "2.5061497688293457\n",
      "2.5054261684417725\n",
      "2.504732131958008\n",
      "2.5040664672851562\n",
      "2.5034267902374268\n",
      "2.502812623977661\n",
      "2.5022218227386475\n",
      "2.5016539096832275\n",
      "2.5011072158813477\n",
      "2.5005807876586914\n",
      "2.5000739097595215\n",
      "2.4995856285095215\n",
      "2.499114513397217\n",
      "2.4986603260040283\n",
      "2.4982221126556396\n",
      "2.4977986812591553\n",
      "2.497390031814575\n",
      "2.496994972229004\n",
      "2.4966132640838623\n",
      "2.496244192123413\n",
      "2.495887041091919\n",
      "2.4955413341522217\n",
      "2.495206356048584\n",
      "2.494882345199585\n",
      "2.4945688247680664\n",
      "2.4942641258239746\n",
      "2.493969440460205\n",
      "2.493683338165283\n",
      "2.493406057357788\n",
      "2.4931366443634033\n",
      "2.492875576019287\n",
      "2.492621421813965\n",
      "2.492375373840332\n",
      "2.492136001586914\n",
      "2.491903066635132\n",
      "2.4916772842407227\n",
      "2.491457223892212\n",
      "2.491243600845337\n",
      "2.4910356998443604\n",
      "2.4908337593078613\n",
      "2.4906368255615234\n",
      "2.490445137023926\n",
      "2.4902586936950684\n",
      "2.490076780319214\n",
      "2.4899003505706787\n",
      "2.4897279739379883\n"
     ]
    }
   ],
   "source": [
    "# summary/training\n",
    "\n",
    "for k in range(100):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() # regularization\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96df5d6f-d550-408a-b7d9-4cfa7dd1ace0",
   "metadata": {},
   "source": [
    "Converged to a loss of about 2.48 after 100 iterations. \\\n",
    "\\\n",
    "About + 0.01*(W**2).mean() component:\n",
    "Pushes the probabilities closer to be uniform.\n",
    "Equivalent to the (N+1) counts in the makemore. The regularization strength is equivalent to the amount we add to N.\n",
    "The higher the strength, the more force the second component exerts, and the more uniform the predictions will be.\n",
    "\n",
    "The next is the sampling part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e9292806-2b67-4b2f-ba56-0c203d582b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wade.\n",
      "akerttelaniges.\n",
      "skertoissycvahlennn.\n",
      "n.\n",
      "camigh.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2149582346)\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        # Before: p = P[idx], so we just grabbed the probability from P\n",
    "        # Now: we get the probability from the neural net\n",
    "        xenc = F.one_hot(torch.tensor([idx]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "        idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[idx])\n",
    "        if idx == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c5611-23bb-4613-b6f7-8c03c1f705d5",
   "metadata": {},
   "source": [
    "Using the same seed number, we achieve the same results and approximately the same loss but in a different way.\n",
    "- The first way: counting the frequency of the bigrams and normalizing\n",
    "- The second way (more flexible): using negative log-likelihood to optimize the counts array by minimizing the loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
