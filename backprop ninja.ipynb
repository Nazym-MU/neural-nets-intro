{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d9ef2fd-cfbb-4a77-b953-e09e691a3eb8",
   "metadata": {},
   "source": [
    "Rewriting loss.backward() manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ce94dd2-abd3-4650-a0cc-068cb7c79046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad966b16-e298-4fbc-9345-d2e956e7845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9018c3f-8c5d-484e-84cf-2e0fd97bba6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34b5ce7f-61b4-4ad4-9972-862bc0b4c8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            idx = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(idx)\n",
    "            context = context[1:] + [idx]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47408246-7e4b-4444-9370-a42cb8d6237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to compare manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item() # true if all elements in both tensors are the same\n",
    "    app = torch.allclose(dt, t.grad) # true if all elements are approximately close\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c9a3465-6d47-49ea-9ff6-9b39407a4f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # number of neurons in the hidden layer\n",
    "\n",
    "g = torch.Generator().manual_seed(456789096543)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1 # useless but let's keep it\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0 # all zeros could mask an incorrect implementation of the backward pass\n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a24c0c2-3299-462c-adad-3de7e5ea3eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# construct a minibatch\n",
    "idx = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[idx], Ytr[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f0798893-9bd6-4030-9095-28d85ff5ea3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3039, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/batch_size*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(batch_size-1)*(bndiff2).sum(0, keepdim=True) # Bessel's correction, dividing by n-1\n",
    "bnvar_inv = (bnvar + 1e05)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Nonlinearity\n",
    "h = torch.tanh(hpreact)\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (F.cross_entropy(logits, Yb) functionality)\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability (so that the highest number is 0 and exp doesn't overflow)\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(batch_size), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, norm_logits, logit_maxes, logits, h, hpreact, bnraw, bnvar_inv,\n",
    "             bnvar, bndiff2, bndiff, hprebn, bnmeani, embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994de699-4d54-47f7-a53d-9568355c2190",
   "metadata": {},
   "source": [
    "Manual backprop, finding derivatives, etc. in the order given in the forward pass\n",
    "\n",
    "---\n",
    "First we find dlogprobs. We see that loss = -logprobs\\[range(batch_size), Yb].mean(). Since we're taking the mean and the derivative with respect to variables at a specific index, the derivative is -1/batch_size.\n",
    "\n",
    "Simpler example: logprobs: a + b + c \\\n",
    "loss = -1/3a - 1/3b - 1/3c \\\n",
    "dloss/da = -1/3, or -1/n\n",
    "\n",
    "Therefore,\\\n",
    "dlogprobs = -1/n\n",
    "\n",
    "---\n",
    "We get logprobs by taking the logarithm of probs (logprobs depends on probs through a log). Therefore, dprobs = dlogprobs * 1/probs.\n",
    "\n",
    "---\n",
    "dcounts_sum_inv = counts must be true because when we take the derivative of probs with respect to dcounts_sum_inv, we're left with counts. But we also need to make sure the dimensions match, so we multiply counts * dprobs (by chain rule) and sum horizontally across rows:\\\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "\n",
    "During the forward pass, for each example in the batch, we have probs\\[i, j] = counts\\[i, j] * counts_sum_inv\\[i].\\\n",
    "counts_sum_inv\\[i] is a scalar that gets multiplied by all elements in row i of counts.\\\n",
    "During the backward pass, dloss/dcounts_sum_inv\\[i] is given by:\n",
    "\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial \\text{counts-sum}_i^{-1}} = \\sum_j \\frac{\\partial \\text{loss}}{\\partial \\text{probs}_{i,j}} \\cdot \\frac{\\partial \\text{probs}_{i,j}}{\\partial \\text{counts-sum}_i^{-1}}$$\n",
    "\n",
    "We know that$\\frac{\\partial \\text{probs}_{i,j}}{\\partial \\text{counts-sum}_i^{-1}} = \\text{counts}_{i, j}$, so:\n",
    "\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial \\text{counts-sum}_i^{-1}} = \\sum_j \\frac{\\partial \\text{loss}}{\\partial \\text{probs}_{i,j}} \\cdot \\text{counts}_{i, j}$$\n",
    "\n",
    "Basically, the gradient is dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) because counts_sum_inv\\[i] is broadcast and used in computing every row i in probs. We must accumulate gradients, so we just sum.\n",
    "\n",
    "---\n",
    "For counts_sum, use the same chain rule and recognize that derivative of counts_sum^{-1} is -counts_sum^{-2}\n",
    "\n",
    "---\n",
    "dcounts = counts_sum_inv * dprobs. No additional summation required because counts_sum_inv is 32 x 1 and dprobs is 32 x 27. As a result we'll get a single vector. That's the first gradient.\n",
    "\n",
    "counts.shape is 32 x 27 and counts_sum.shape is 32 x 1.\\\n",
    "$$\\frac{\\partial \\text{counts-sum}_i}{\\text{counts}_{i, j}} = 1$$\n",
    "because counts_sum is about summing counts along rows. The gradient along rows other than row $i$ would be 0 (no contribution). Therefore, when we evaluate the gradient of the loss:\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial \\text{counts}_{i,j}} = \\frac{\\partial \\text{loss}}{\\partial \\text{counts-sum}_{i}} \\cdot \\frac{\\partial \\text{counts-sum}_i}{\\partial \\text{counts}_{i, j}} = \\frac{\\partial \\text{loss}}{\\partial \\text{counts-sum}_{i}} \\cdot 1$$\n",
    "dcounts_sum is the partial derivative in the result.\\\n",
    "torch.ones_like(counts) * dcounts_sum broadcasts dcounts_sum\\[i] to all elements in row i.\n",
    "\n",
    "---\n",
    "Now we take the derivative of the loss with respect to norm_logits. Chain rule + recognizing the derivative of the exponential is unchanged.\\\n",
    "As a result, we get dnorm_logits = e^{norm_logits} * dcounts = counts * dcounts\n",
    "\n",
    "---\n",
    "norm_logits depends on logits and logit_maxes. Check the shapes again:\\\n",
    "norm_logits is 32 x 27, logits is 32 x 27, and logit_maxes is 32 x 1.\\\n",
    "norm_logits = logits - logit_maxes\\\n",
    "When we find dlogits, we use the chain rule and the fact that the derivative with respect to logits is 1 to get dlogits = dnorm_logits.\n",
    "For logit_maxes, it is -1 * dnorm_logits, but we also need to sum along the rows. Lesssgoooo, I'm finally understanding this.\n",
    "\n",
    "---\n",
    "Second branch of dlogits:\\\n",
    "logit_maxes = logits.max(1, keepdim=True).values\\\n",
    "The local derivative is 1 (derivative of logit_maxes with respect to logits). We only need to mask one entry per row to be 1 (the index of the maximum logit). One solution is to use F.one_hot(logits.max(1).indices, num_classes=logits.shape\\[1]). There are 27 categories -> num_classes. Using the chain rule, we multiply by dlogit_maxes, which is a column vector, so it will broadcast. Whichever of the bits is turned on using F.one_hot() will also be multiplied by the gradient with respect to logit_maxes.\n",
    "\n",
    "---\n",
    "We have logits = h @ W2 + b2. Taking the derivative with respect to h, we get W2 * dlogits. However, let's look at the shapes:\\\n",
    "dlogits -> 32 x 27, h -> 32 x 64, W2 -> 64 x 27, b2 -> 27\n",
    "\n",
    "bias vector will become a row vector in the broadcasting and replicate vertically.\\\n",
    "Consider a simple example d = a @ b + c:\n",
    "$$\\begin{bmatrix} d_{11} & d_{12} \\\\ d_{21} & d_{22} \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} \\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{bmatrix} + \\begin{bmatrix} c_{1} & c_{2} \\\\ c_{1} & c_{2} \\end{bmatrix}$$\n",
    "=>\n",
    "$$d_{11} = a_{11}b_{11} + a_{12}b_{21} + c_1 \\qquad d_{12} = a_{11}b_{12} + a_{12}b_{22} + c_2$$\n",
    "$$d_{21} = a_{21}b_{11} + a_{22}b_{21} + c_1 \\qquad d_{22} = a_{21}b_{12} + a_{22}b_{22} + c_2$$\n",
    "Taking the derivative of the loss with respect to $a_{11}, a_{12}, a_{21}, a_{22}$, we find that it's a simple matrix multiplication:\n",
    "$$\\frac{\\partial L}{\\partial a} = \\frac{dL}{dd} \\times b^T \\qquad \\frac{\\partial L}{\\partial b} = a^T \\times \\frac{dL}{dd} \\qquad \\frac{\\partial L}{\\partial c} = \\frac{dL}{dd} \\cdot \\text{sum(0)}$$\n",
    "sum(0) means sum along the columns.\\\n",
    "Or just make sure that the dimensions work out.\n",
    "\n",
    "---\n",
    "h = torch.tanh(hpreact)\n",
    "d/dx tanh(x) = sech^2 x <- not very helpful. If $a = \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$, then a simpler formula is $\\frac{da}{dz} = 1 - a^2$. Then, using the chain rule, dhpreact = (1.0 - h**2) * dh.\n",
    "\n",
    "---\n",
    "Then we want to backpropagate into bngain, bnraw, and bnbias.\\\n",
    "hpreact: 32 x 64, bngain: 1 x 64, bnraw: 32 x 64, bnbias: 1 x 64 \\\n",
    "We're just shifting and scaling, so it makes sense that bnraw has the same dimensions as hpreact.\n",
    "\n",
    "Use the chain rule: dbngain = bnraw * dhpreact, but dimensions must be 1 x 64 and currently it is 64 x 64, so we need to sum across the columns. Same for dbnraw, but we don't need to sum because of broadcasting. For dbnbias, we need dimension 1 x 64, so we sum dhpreact across the columns\n",
    "\n",
    "---\n",
    "Backpropagate through bndiff and bnvar_inv. Similar to other expressions, use the chain rule and match dimensions.\\\n",
    "Dimensions are: bnraw - 32 x 64, bndiff - 32 x 64, bnvar_inv - 1 x 64\\\n",
    "dbndiff = bnvar_inv * dbnraw (dimensions preserved)\\\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0)\n",
    "\n",
    "---\n",
    "Similarly:\\\n",
    "bnvar_inv = (bnvar + 1e05)^-0.5\\\n",
    "dbnvar = -0.5(bnvar + 1e05)^-1.5 * dbnvar_inv # Yayy, correct\n",
    "\n",
    "bnvar = 1/(batch_size-1)*(bndiff2).sum(0)\\\n",
    "dbndiff2 = 1/(batch_size-1)*torch.ones_like(bndiff2) * dbnvar because the local derivative is just an array of 1s scaled by 1/n-1\n",
    "\n",
    "bndiff2 = bndiff**2\\\n",
    "dbndiff = 2*bndiff * dbndiff2\n",
    "\n",
    "bndiff = hprebn - bnmeani\\\n",
    "Shapes: bndiff - 32 x 64, hprebn - 32 x 64, bnmeani - 1 x 64\\\n",
    "dhprebn = 1 * dbndiff\\\n",
    "dbnmeani = (-dbndiff).sum(0) # sum across columns to match dimensions\n",
    "\n",
    "bnmeani = 1/batch_size*hprebn.sum(0, keepdim=True)\\\n",
    "dhprebn = 1/batch_size * dbnmeani\n",
    "\n",
    "hprebn = embcat @ W1 + b1\\\n",
    "Shapes: W1 - 30 x 64, hprebn - 32 x 64, embcat - 32 x 30, b1 - 64\\\n",
    "dembcat = dhprebn @ W1.T \\\n",
    "dW1 = embcat.T @ dhprebn \\\n",
    "db1 = dhprebn.sum(0\n",
    "\n",
    "embcat = emb.view(emb.shape\\[0], -1)\\\n",
    "Shapes: embcat - 32 x 30, emb - 32 x 3 x 10\\\n",
    "To revert, we rerepresent it in the original shape: dembcat.view(emb.shape\n",
    "\n",
    "---\n",
    "Indexing in the forward pass: emb = C\\[Xb]\\\n",
    "Shapes: emb - 32 x 3 x 10 (32 examples, 3 characters, 10d embedding), C - 27 x 10 (lookup table with 27 chatracters, each 10d), Xb - 32 x 3 (32 examples, context length of 3)\\\n",
    "dC must accumulate the gradients. We iterate through all examples in the batch and each position in the context window, get the vocabulary index used at that position, and add the gradient to dC\\[idx]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7b50eea-df34-4146-8aec-d00abd4e79f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcounts_sum_inv | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcounts_sum     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcounts         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dnorm_logits    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dlogit_maxes    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dlogits         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dh              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dW2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "db2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dhpreact        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbngain         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnraw          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnbias         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnvar_inv      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnvar          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbndiff2        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbndiff         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dhprebn         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnmeani        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dembcat         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dW1             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "db1             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "demb            | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dC              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(batch_size), Yb] = -1.0/batch_size\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "\n",
    "dprobs = 1.0/probs * dlogprobs\n",
    "cmp('probs', dprobs, probs)\n",
    "\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "cmp('dcounts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "\n",
    "dcounts_sum = -counts_sum**-2 * dcounts_sum_inv\n",
    "cmp('dcounts_sum', dcounts_sum, counts_sum)\n",
    "\n",
    "dcounts = counts_sum_inv * dprobs # first gradient with respect to dcounts\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum # second gradient\n",
    "cmp('dcounts', dcounts, counts)\n",
    "\n",
    "dnorm_logits = norm_logits.exp() * dcounts\n",
    "cmp('dnorm_logits', dnorm_logits, norm_logits)\n",
    "\n",
    "dlogits = dnorm_logits.clone() # this is no the final derivative for the logits because there's more\n",
    "\n",
    "dlogit_maxes = -dnorm_logits.sum(1, keepdim=True)\n",
    "cmp('dlogit_maxes', dlogit_maxes, logit_maxes)\n",
    "\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes # second branch of dlogits\n",
    "cmp('dlogits', dlogits, logits)\n",
    "\n",
    "dh = dlogits @ W2.T\n",
    "cmp('dh', dh, h)\n",
    "\n",
    "dW2 = h.T @ dlogits\n",
    "cmp('dW2', dW2, W2)\n",
    "\n",
    "db2 = dlogits.sum(0)\n",
    "cmp('db2', db2, b2)\n",
    "\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "cmp('dhpreact', dhpreact, hpreact)\n",
    "\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "cmp('dbngain', dbngain, bngain)\n",
    "\n",
    "dbnraw = bngain * dhpreact # dimension preserved\n",
    "cmp('dbnraw', dbnraw, bnraw)\n",
    "\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "cmp('dbnbias', dbnbias, bnbias)\n",
    "\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# cmp('dbndiff', dbndiff, bndiff)\n",
    "cmp('dbnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "\n",
    "dbnvar = -0.5*(bnvar + 1e05)**-1.5 * dbnvar_inv\n",
    "cmp('dbnvar', dbnvar, bnvar)\n",
    "\n",
    "dbndiff2 = 1/(batch_size-1)*torch.ones_like(bndiff2) * dbnvar\n",
    "cmp('dbndiff2', dbndiff2, bndiff2)\n",
    "\n",
    "dbndiff += 2*bndiff * dbndiff2 # second branch of dbndiff\n",
    "cmp('dbndiff', dbndiff, bndiff)\n",
    "\n",
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (-dbndiff).sum(0)\n",
    "dhprebn += (1/batch_size) * torch.ones_like(hprebn) * dbnmeani # second branch of dhprebn\n",
    "cmp('dhprebn', dhprebn, hprebn)\n",
    "cmp('dbnmeani', dbnmeani, bnmeani)\n",
    "\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0, keepdim=True)\n",
    "cmp('dembcat', dembcat, embcat)\n",
    "cmp('dW1', dW1, W1)\n",
    "cmp('db1', db1, b1)\n",
    "\n",
    "demb = dembcat.view(emb.shape)\n",
    "cmp('demb', demb, emb)\n",
    "\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        idx = Xb[k, j]\n",
    "        dC[idx] += demb[k, j]\n",
    "\n",
    "cmp('dC', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6eeaa-bb27-47c7-a31d-29e378bd8bb2",
   "metadata": {},
   "source": [
    "Exercise 2: simplify the loss calculation\n",
    "\n",
    "\n",
    "\n",
    "$$\\text{loss} = -\\log P_y = -\\log \\frac{e^{l_y}}{\\sum_j e^{l_j}}$$\n",
    "where the last expression arises from applying softmax (raising to the power of the exponential and normalizing)\n",
    "\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial l_i} = \\frac{\\partial}{\\partial l_i}\\left[-\\log \\frac{e^{l_y}}{\\sum_j e^{l_j}}\\right] = -\\frac{\\sum_j e^{l_j}}{e^{l_y}} \\cdot \\frac{\\partial}{\\partial l_i}\\left[ \\frac{e^{l_y}}{\\sum_j e^{l_j}} \\right]$$\n",
    "\n",
    "When $i \\neq y$:\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial l_i} = -\\frac{\\sum_j e^{l_j}}{e^{l_y}} \\left[0 \\cdot \\frac{1}{\\sum_j e^{l_j}} - e^{l_y} \\cdot \\frac{e^{l_i}}{(\\sum_j e^{l_j})^2} \\right] = \\frac{e^{l_i}}{\\sum_j e^{l_j}} = P_i$$\n",
    "When $i = y$:\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial l_i} = -\\frac{\\sum_j e^{l_j}}{e^{l_y}} \\left[e^{l_y} \\cdot \\frac{1}{\\sum_j e^{l_j}} - e^{l_y} \\cdot \\frac{e^{l_i}}{(\\sum_j e^{l_j})^2} \\right] = \\frac{e^{l_i}}{\\sum_j e^{l_j}} - 1 = P_i - 1$$\n",
    "\n",
    "Simplifies to $P_i$ and $P_i - 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "81534407-4a4b-4789-bec7-cfcd7eb7ba95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 8.381903171539307e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "dlogits = F.softmax(logits, 1) # apply softmax along the rows\n",
    "dlogits[range(batch_size), Yb] -= 1  # subtract 1 from the gradient\n",
    "dlogits /= batch_size # divide the gradient by the batch size (take the average)\n",
    "\n",
    "cmp('logits', dlogits, logits) # only approximate is true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e34fd0-67f5-4be2-988a-2bcfd129e97e",
   "metadata": {},
   "source": [
    "Exercise 3: backprop through batchnorm but all in one go\n",
    "\n",
    "$$\\mu = \\frac{1}{m} \\sum_i x_i \\qquad \\sigma^2 = \\frac{1}{m-1}\\sum_i (x_i-\\mu)^2$$\n",
    "$$\\hat{x}_i=\\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} \\qquad y_i = \\gamma \\hat{x}_i + \\beta$$\n",
    "We know $\\partial L/\\partial y_i$ and need to find $\\partial L/\\partial x_i$\n",
    "$$\\frac{\\partial L}{\\partial \\hat x_i}=\\gamma \\cdot \\frac{\\partial L}{\\partial y_i}$$\n",
    "$$\\frac{\\partial L}{\\partial \\hat \\sigma^2}=\\sum_i \\frac{\\partial L}{\\partial \\hat x_i}\\frac{\\partial \\hat x_i}{\\partial \\sigma^2} = \\gamma \\sum_i \\frac{\\partial L}{\\partial y_i} \\cdot \\frac{\\partial}{\\partial \\sigma^2}\\left[ (x_i - \\mu)(\\sigma^2 + \\varepsilon)^{-1/2} \\right] = -\\frac{1}{2} \\gamma \\sum_i \\frac{\\partial L}{\\partial y_i} \\cdot (x_i - \\mu)(\\sigma^2 + \\varepsilon)^{-3/2}$$\n",
    "The next one has two arrows because it appears in two equations: one from $\\hat x$ and one from $\\sigma^2$:\n",
    "$$\\frac{\\partial L}{\\partial \\mu} = \\sum_i \\frac{\\partial L}{\\partial \\hat x_i}\\frac{\\partial \\hat x_i}{\\partial \\mu} + \\frac{\\partial L}{\\partial \\sigma^2} \\frac{\\partial \\sigma^2}{\\partial \\mu}= \\gamma \\sum_i \\frac{\\partial L}{\\partial y_i} \\cdot (-(\\sigma^2 + \\varepsilon)^{-1/2}) + \\frac{\\partial L}{\\partial \\sigma^2}\\cdot 0 = -\\gamma \\sum_i \\frac{\\partial L}{\\partial y_i} \\cdot (\\sigma^2 + \\varepsilon)^{-1/2}$$\n",
    "Finally, to derive $\\partial L/\\partial x_i$, we first need to recognize that there are three arrows eminating from $x_i$: to $\\mu, \\sigma^2$, and $\\hat x$. Use the chain rule and add up the contributions from those three:\n",
    "$$\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial \\hat x_i} \\frac{\\partial \\hat x_i}{x_i} + \\frac{\\partial L}{\\partial \\mu} \\frac{\\partial \\mu}{x_i} + \\frac{\\partial L}{\\partial \\sigma^2} \\frac{\\partial \\sigma^2}{x_i} = \\gamma \\frac{\\partial L}{\\partial y_i}(\\sigma^2 + \\varepsilon)^{-1/2}  -\\frac{\\gamma}{m} \\sum_j \\frac{\\partial L}{\\partial y_i} \\cdot (\\sigma^2 + \\varepsilon)^{-1/2} -\\frac{1}{2} \\gamma \\sum_j \\frac{\\partial L}{\\partial y_i} \\cdot (x_i - \\mu)(\\sigma^2 + \\varepsilon)^{-3/2} \\cdot \\frac{2}{m-1} (x_i - \\mu) =$$\n",
    "$$=\\frac{\\gamma(\\sigma^2 + \\varepsilon)^{-1/2}}{m} \\left[ m\\frac{\\partial L}{\\partial y_i} - \\sum_j \\frac{\\partial L}{\\partial y_j} - \\frac{m}{m-1} \\hat x_i \\sum_j \\frac{\\partial L}{\\partial y_j} \\hat{x_j} \\right]$$\n",
    "\n",
    "I skipped some derivation steps, but implementing this expression in code, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e602afe1-3d19-4d97-b93a-bf66c58254fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 3.637978807091713e-12\n"
     ]
    }
   ],
   "source": [
    "# m is batch_size\n",
    "dhprebn = bngain * bnvar_inv/batch_size * (batch_size*dhpreact - dhpreact.sum(0) - batch_size/(batch_size-1) * bnraw * (dhpreact * bnraw).sum(0))\n",
    "cmp('hprebn', dhprebn, hprebn) # approximation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
